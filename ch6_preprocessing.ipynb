{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch6_preprocessing.ipynb",
      "provenance": [],
      "mount_file_id": "12ESNbMdhRsqiy4ZoDvZiFSbUR9FMBlEB",
      "authorship_tag": "ABX9TyMzx41xAKABW5qIzxLdxHla",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/silverstar0727/NLP_study/blob/main/ch6_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrFyk5mEZtPX",
        "outputId": "278252ed-6001-4100-ea72-7c7b87dfd5db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content/drive/My Drive/ml/nlp/book/ch6"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/ml/nlp/book/ch6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqAJ9AshZtM1"
      },
      "source": [
        "!pip install -q konlpy\n",
        "!pip install -q preprocessing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GN8holOxZtKT"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "\n",
        "FILTERS = \"([~.,!?\\\"':;)(])\"\n",
        "PAD = \"<PAD>\"\n",
        "STD = \"<SOS>\"\n",
        "END = \"<END>\"\n",
        "UNK = \"<UNK>\"\n",
        "\n",
        "PAD_INDEX = 0\n",
        "STD_INDEX = 1\n",
        "END_INDEX = 2\n",
        "UNK_INDEX = 3\n",
        "\n",
        "MARKER = [PAD, STD, END, UNK]\n",
        "CHANGE_FILTER = re.compile(FILTERS)\n",
        "\n",
        "MAX_SEQUENCE = 25\n",
        "\n",
        "\n",
        "def load_data(path):\n",
        "    # 판다스를 통해서 데이터를 불러온다.\n",
        "    data_df = pd.read_csv(path, header=0)\n",
        "    # 질문과 답변 열을 가져와 question과 answer에 넣는다.\n",
        "    question, answer = list(data_df['Q']), list(data_df['A'])\n",
        "\n",
        "    return question, answer\n",
        "\n",
        "\n",
        "def data_tokenizer(data):\n",
        "    # 토크나이징 해서 담을 배열 생성\n",
        "    words = []\n",
        "    for sentence in data:\n",
        "        # FILTERS = \"([~.,!?\\\"':;)(])\"\n",
        "        # 위 필터와 같은 값들을 정규화 표현식을\n",
        "        # 통해서 모두 \"\" 으로 변환 해주는 부분이다.\n",
        "        sentence = re.sub(CHANGE_FILTER, \"\", sentence)\n",
        "        for word in sentence.split():\n",
        "            words.append(word)\n",
        "    # 토그나이징과 정규표현식을 통해 만들어진\n",
        "    # 값들을 넘겨 준다.\n",
        "    return [word for word in words if word]\n",
        "\n",
        "\n",
        "def prepro_like_morphlized(data):\n",
        "    morph_analyzer = Okt()\n",
        "    result_data = list()\n",
        "    for seq in tqdm(data):\n",
        "        morphlized_seq = \" \".join(morph_analyzer.morphs(seq.replace(' ', '')))\n",
        "        result_data.append(morphlized_seq)\n",
        "\n",
        "    return result_data\n",
        "\n",
        "\n",
        "def load_vocabulary(path, vocab_path, tokenize_as_morph=False):\n",
        "    # 사전을 담을 배열 준비한다.\n",
        "    vocabulary_list = []\n",
        "    # 사전을 구성한 후 파일로 저장 진행한다.\n",
        "    # 그 파일의 존재 유무를 확인한다.\n",
        "    if not os.path.exists(vocab_path):\n",
        "        # 이미 생성된 사전 파일이 존재하지 않으므로\n",
        "        # 데이터를 가지고 만들어야 한다.\n",
        "        # 그래서 데이터가 존재 하면 사전을 만들기 위해서\n",
        "        # 데이터 파일의 존재 유무를 확인한다.\n",
        "        if (os.path.exists(path)):\n",
        "            # 데이터가 존재하니 판단스를 통해서\n",
        "            # 데이터를 불러오자\n",
        "            data_df = pd.read_csv(path, encoding='utf-8')\n",
        "            # 판다스의 데이터 프레임을 통해서\n",
        "            # 질문과 답에 대한 열을 가져 온다.\n",
        "            question, answer = list(data_df['Q']), list(data_df['A'])\n",
        "            if tokenize_as_morph:  # 형태소에 따른 토크나이져 처리\n",
        "                question = prepro_like_morphlized(question)\n",
        "                answer = prepro_like_morphlized(answer)\n",
        "            data = []\n",
        "            # 질문과 답변을 extend을\n",
        "            # 통해서 구조가 없는 배열로 만든다.\n",
        "            data.extend(question)\n",
        "            data.extend(answer)\n",
        "            # 토큰나이져 처리 하는 부분이다.\n",
        "            words = data_tokenizer(data)\n",
        "            # 공통적인 단어에 대해서는 모두\n",
        "            # 필요 없으므로 한개로 만들어 주기 위해서\n",
        "            # set해주고 이것들을 리스트로 만들어 준다.\n",
        "            words = list(set(words))\n",
        "            # 데이터 없는 내용중에 MARKER를 사전에\n",
        "            # 추가 하기 위해서 아래와 같이 처리 한다.\n",
        "            # 아래는 MARKER 값이며 리스트의 첫번째 부터\n",
        "            # 순서대로 넣기 위해서 인덱스 0에 추가한다.\n",
        "            # PAD = \"<PADDING>\"\n",
        "            # STD = \"<START>\"\n",
        "            # END = \"<END>\"\n",
        "            # UNK = \"<UNKNWON>\"\n",
        "            words[:0] = MARKER\n",
        "        # 사전을 리스트로 만들었으니 이 내용을\n",
        "        # 사전 파일을 만들어 넣는다.\n",
        "        with open(vocab_path, 'w', encoding='utf-8') as vocabulary_file:\n",
        "            for word in words:\n",
        "                vocabulary_file.write(word + '\\n')\n",
        "\n",
        "    # 사전 파일이 존재하면 여기에서\n",
        "    # 그 파일을 불러서 배열에 넣어 준다.\n",
        "    with open(vocab_path, 'r', encoding='utf-8') as vocabulary_file:\n",
        "        for line in vocabulary_file:\n",
        "            vocabulary_list.append(line.strip())\n",
        "\n",
        "    # 배열에 내용을 키와 값이 있는\n",
        "    # 딕셔너리 구조로 만든다.\n",
        "    char2idx, idx2char = make_vocabulary(vocabulary_list)\n",
        "    # 두가지 형태의 키와 값이 있는 형태를 리턴한다.\n",
        "    # (예) 단어: 인덱스 , 인덱스: 단어)\n",
        "    return char2idx, idx2char, len(char2idx)\n",
        "\n",
        "\n",
        "def make_vocabulary(vocabulary_list):\n",
        "    # 리스트를 키가 단어이고 값이 인덱스인\n",
        "    # 딕셔너리를 만든다.\n",
        "    char2idx = {char: idx for idx, char in enumerate(vocabulary_list)}\n",
        "    # 리스트를 키가 인덱스이고 값이 단어인\n",
        "    # 딕셔너리를 만든다.\n",
        "    idx2char = {idx: char for idx, char in enumerate(vocabulary_list)}\n",
        "    # 두개의 딕셔너리를 넘겨 준다.\n",
        "    return char2idx, idx2char\n",
        "\n",
        "\n",
        "def enc_processing(value, dictionary, tokenize_as_morph=False):\n",
        "    # 인덱스 값들을 가지고 있는\n",
        "    # 배열이다.(누적된다.)\n",
        "    sequences_input_index = []\n",
        "    # 하나의 인코딩 되는 문장의\n",
        "    # 길이를 가지고 있다.(누적된다.)\n",
        "    sequences_length = []\n",
        "    # 형태소 토크나이징 사용 유무\n",
        "    if tokenize_as_morph:\n",
        "        value = prepro_like_morphlized(value)\n",
        "\n",
        "    # 한줄씩 불어온다.\n",
        "    for sequence in value:\n",
        "        # FILTERS = \"([~.,!?\\\"':;)(])\"\n",
        "        # 정규화를 사용하여 필터에 들어 있는\n",
        "        # 값들을 \"\" 으로 치환 한다.\n",
        "        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n",
        "        # 하나의 문장을 인코딩 할때\n",
        "        # 가지고 있기 위한 배열이다.\n",
        "        sequence_index = []\n",
        "        # 문장을 스페이스 단위로\n",
        "        # 자르고 있다.\n",
        "        for word in sequence.split():\n",
        "            # 잘려진 단어들이 딕셔너리에 존재 하는지 보고\n",
        "            # 그 값을 가져와 sequence_index에 추가한다.\n",
        "            if dictionary.get(word) is not None:\n",
        "                sequence_index.extend([dictionary[word]])\n",
        "            # 잘려진 단어가 딕셔너리에 존재 하지 않는\n",
        "            # 경우 이므로 UNK(2)를 넣어 준다.\n",
        "            else:\n",
        "                sequence_index.extend([dictionary[UNK]])\n",
        "        # 문장 제한 길이보다 길어질 경우 뒤에 토큰을 자르고 있다.\n",
        "        if len(sequence_index) > MAX_SEQUENCE:\n",
        "            sequence_index = sequence_index[:MAX_SEQUENCE]\n",
        "        # 하나의 문장에 길이를 넣어주고 있다.\n",
        "        sequences_length.append(len(sequence_index))\n",
        "        # max_sequence_length보다 문장 길이가\n",
        "        # 작다면 빈 부분에 PAD(0)를 넣어준다.\n",
        "        sequence_index += (MAX_SEQUENCE - len(sequence_index)) * [dictionary[PAD]]\n",
        "        # 인덱스화 되어 있는 값을\n",
        "        # sequences_input_index에 넣어 준다.\n",
        "        sequences_input_index.append(sequence_index)\n",
        "    # 인덱스화된 일반 배열을 넘파이 배열로 변경한다.\n",
        "    # 이유는 텐서플로우 dataset에 넣어 주기 위한\n",
        "    # 사전 작업이다.\n",
        "    # 넘파이 배열에 인덱스화된 배열과\n",
        "    # 그 길이를 넘겨준다.\n",
        "    return np.asarray(sequences_input_index), sequences_length\n",
        "\n",
        "\n",
        "def dec_output_processing(value, dictionary, tokenize_as_morph=False):\n",
        "    # 인덱스 값들을 가지고 있는\n",
        "    # 배열이다.(누적된다)\n",
        "    sequences_output_index = []\n",
        "    # 하나의 디코딩 입력 되는 문장의\n",
        "    # 길이를 가지고 있다.(누적된다)\n",
        "    sequences_length = []\n",
        "    # 형태소 토크나이징 사용 유무\n",
        "    if tokenize_as_morph:\n",
        "        value = prepro_like_morphlized(value)\n",
        "    # 한줄씩 불어온다.\n",
        "    for sequence in value:\n",
        "        # FILTERS = \"([~.,!?\\\"':;)(])\"\n",
        "        # 정규화를 사용하여 필터에 들어 있는\n",
        "        # 값들을 \"\" 으로 치환 한다.\n",
        "        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n",
        "        # 하나의 문장을 디코딩 할때 가지고\n",
        "        # 있기 위한 배열이다.\n",
        "        sequence_index = []\n",
        "        # 디코딩 입력의 처음에는 START가 와야 하므로\n",
        "        # 그 값을 넣어 주고 시작한다.\n",
        "        # 문장에서 스페이스 단위별로 단어를 가져와서 딕셔너리의\n",
        "        # 값인 인덱스를 넣어 준다.\n",
        "        sequence_index = [dictionary[STD]] + [dictionary[word] if word in dictionary else dictionary[UNK] for word in sequence.split()]\n",
        "        # 문장 제한 길이보다 길어질 경우 뒤에 토큰을 자르고 있다.\n",
        "        if len(sequence_index) > MAX_SEQUENCE:\n",
        "            sequence_index = sequence_index[:MAX_SEQUENCE]\n",
        "        # 하나의 문장에 길이를 넣어주고 있다.\n",
        "        sequences_length.append(len(sequence_index))\n",
        "        # max_sequence_length보다 문장 길이가\n",
        "        # 작다면 빈 부분에 PAD(0)를 넣어준다.\n",
        "        sequence_index += (MAX_SEQUENCE - len(sequence_index)) * [dictionary[PAD]]\n",
        "        # 인덱스화 되어 있는 값을\n",
        "        # sequences_output_index 넣어 준다.\n",
        "        sequences_output_index.append(sequence_index)\n",
        "    # 인덱스화된 일반 배열을 넘파이 배열로 변경한다.\n",
        "    # 이유는 텐서플로우 dataset에 넣어 주기 위한\n",
        "    # 사전 작업이다.\n",
        "    # 넘파이 배열에 인덱스화된 배열과 그 길이를 넘겨준다.\n",
        "    return np.asarray(sequences_output_index), sequences_length\n",
        "\n",
        "\n",
        "def dec_target_processing(value, dictionary, tokenize_as_morph=False):\n",
        "    # 인덱스 값들을 가지고 있는\n",
        "    # 배열이다.(누적된다)\n",
        "    sequences_target_index = []\n",
        "    # 형태소 토크나이징 사용 유무\n",
        "    if tokenize_as_morph:\n",
        "        value = prepro_like_morphlized(value)\n",
        "    # 한줄씩 불어온다.\n",
        "    for sequence in value:\n",
        "        # FILTERS = \"([~.,!?\\\"':;)(])\"\n",
        "        # 정규화를 사용하여 필터에 들어 있는\n",
        "        # 값들을 \"\" 으로 치환 한다.\n",
        "        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n",
        "        # 문장에서 스페이스 단위별로 단어를 가져와서\n",
        "        # 딕셔너리의 값인 인덱스를 넣어 준다.\n",
        "        # 디코딩 출력의 마지막에 END를 넣어 준다.\n",
        "        sequence_index = [dictionary[word] if word in dictionary else dictionary[UNK] for word in sequence.split()]\n",
        "        # 문장 제한 길이보다 길어질 경우 뒤에 토큰을 자르고 있다.\n",
        "        # 그리고 END 토큰을 넣어 준다\n",
        "        if len(sequence_index) >= MAX_SEQUENCE:\n",
        "            sequence_index = sequence_index[:MAX_SEQUENCE - 1] + [dictionary[END]]\n",
        "        else:\n",
        "            sequence_index += [dictionary[END]]\n",
        "        # max_sequence_length보다 문장 길이가\n",
        "        # 작다면 빈 부분에 PAD(0)를 넣어준다.\n",
        "        sequence_index += (MAX_SEQUENCE - len(sequence_index)) * [dictionary[PAD]]\n",
        "        # 인덱스화 되어 있는 값을\n",
        "        # sequences_target_index에 넣어 준다.\n",
        "        sequences_target_index.append(sequence_index)\n",
        "    # 인덱스화된 일반 배열을 넘파이 배열로 변경한다.\n",
        "    # 이유는 텐서플로우 dataset에 넣어 주기 위한 사전 작업이다.\n",
        "    # 넘파이 배열에 인덱스화된 배열과 그 길이를 넘겨준다.\n",
        "    return np.asarray(sequences_target_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNd2JT1fAo-Y"
      },
      "source": [
        "os.remove('vocabulary.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjCviPoOZtFg"
      },
      "source": [
        "PATH = 'ChatBotData.csv'\n",
        "VOCAB_PATH = 'vocabulary.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LX08d1AfZtDA"
      },
      "source": [
        "inputs, outputs = load_data(PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--cI7IArZtAa",
        "outputId": "415be3bd-cd06-427f-c07f-2dab5eb4f410",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "char2idx, idx2char, vocab_size = load_vocabulary(PATH, VOCAB_PATH, tokenize_as_morph=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 11823/11823 [00:40<00:00, 294.32it/s]\n",
            "100%|██████████| 11823/11823 [00:43<00:00, 270.50it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJUz91XDZs-D",
        "outputId": "a393025f-9d10-4724-bb14-a5a4dbbad2eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "index_inputs, input_seq_len = enc_processing(inputs, char2idx, tokenize_as_morph=True)\n",
        "index_outputs, output_seq_len = dec_output_processing(outputs, char2idx, tokenize_as_morph=True)\n",
        "index_targets = dec_target_processing(outputs, char2idx, tokenize_as_morph=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 11823/11823 [00:32<00:00, 358.30it/s]\n",
            "100%|██████████| 11823/11823 [00:43<00:00, 270.18it/s]\n",
            "100%|██████████| 11823/11823 [00:43<00:00, 269.20it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVbYuHuqZs7j"
      },
      "source": [
        "data_configs = {}\n",
        "data_configs['char2idx'] = char2idx\n",
        "data_configs['idx2char'] = idx2char\n",
        "data_configs['vocab_size'] = vocab_size\n",
        "data_configs['pad_symbol'] = PAD\n",
        "data_configs['std_symbol'] = STD\n",
        "data_configs['end_symbol'] = END\n",
        "data_configs['unk_symbol'] = UNK"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lWtfE9VZs5b"
      },
      "source": [
        "TRAIN_INPUTS = 'train_inputs.npy'\n",
        "TRAIN_OUTPUTS = 'train_outputs.npy'\n",
        "TRAIN_TARGETS = 'train_targets.npy'\n",
        "DATA_CONFIGS = 'data_configs.json'\n",
        "\n",
        "np.save(open(TRAIN_INPUTS, 'wb'), index_inputs)\n",
        "np.save(open(TRAIN_OUTPUTS , 'wb'), index_outputs)\n",
        "np.save(open(TRAIN_TARGETS , 'wb'), index_targets)\n",
        "\n",
        "json.dump(data_configs, open(DATA_CONFIGS, 'w'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOt2xvP7aK5b"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}