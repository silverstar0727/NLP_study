{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "실습 자료만들기.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNh3Y87mU3h/Gnnh5R4XroG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/silverstar0727/NLP_study/blob/main/%EC%8B%A4%EC%8A%B5_%EC%9E%90%EB%A3%8C%EB%A7%8C%EB%93%A4%EA%B8%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExzFv22mj_6v",
        "outputId": "1ca0eb0f-5c98-4389-9f35-f488a57d0223",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "cd /content/drive/My Drive/ml/nlp/book/ch6"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/ml/nlp/book/ch6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2r0F3jaxkH0U",
        "outputId": "22b889d7-663a-4499-e44a-1c2d1988a516",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "!pip install -q konlpy\n",
        "!pip install -q preprocessing"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 19.4MB 1.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 92kB 12.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.8MB 45.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 358kB 4.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.4MB 9.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 26.3MB/s \n",
            "\u001b[?25h  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sWLSSYE8oxo"
      },
      "source": [
        "# 필요한 라이브러리 임포트\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from konlpy.tag import Twitter\n",
        "import pandas as pd\n",
        "import enum\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from preprocessing import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wYkwqfgSv2n"
      },
      "source": [
        "SEED_NUM = 1234\n",
        "tf.random.set_seed(SEED_NUM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tF1udyFtSv9B",
        "outputId": "a8600a4c-169c-416e-e945-bd7fb2ec2008",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "char2idx = prepro_configs['char2idx']\n",
        "end_index = prepro_configs['end_symbol']\n",
        "model_name = 'transformer'\n",
        "vocab_size = prepro_configs['vocab_size']\n",
        "BATCH_SIZE = 64\n",
        "MAX_SEQUENCE = 25\n",
        "EPOCHS = 35\n",
        "VALID_SPLIT = 0.1\n",
        "\n",
        "# 각종 arguments 정의\n",
        "kargs = {'model_name': model_name,\n",
        "         'num_layers': 2,\n",
        "         'd_model': 512,\n",
        "         'num_heads': 8,\n",
        "         'dff': 2048,\n",
        "         'input_vocab_size': vocab_size,\n",
        "         'target_vocab_size': vocab_size,\n",
        "         'maximum_position_encoding': MAX_SEQUENCE,\n",
        "         'end_token_idx': char2idx[end_index],\n",
        "         'rate': 0.1\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "98 99 100\n",
            "bcd\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDRrhhUTdrPk"
      },
      "source": [
        "## mask 만들기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RTOfyv7SwBg"
      },
      "source": [
        "# sequence를 받아서 패딩\n",
        "def create_padding_mask(seq):\n",
        "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32) # seq중 0을 기준으로 bool로 바꾼후 0으로 casting\n",
        "\n",
        "  return seq[:, tf.mewaxis, tf.newaxis, :] # (batch_size, 1, 1, seq_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpyOPIfjOgM4"
      },
      "source": [
        "def create_look_ahead_mask(size): # mask의 사이즈를 parameter로 받음\n",
        "  mask = 1 - tf.linalg.band_part(tf.ones(size, size), -1, 0) # 하삼각행렬로 변환\n",
        "\n",
        "  return mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pFhs4FaOgPQ"
      },
      "source": [
        "def create_masks(inp, tar):\n",
        "  enc_padding_mask = create_padding_mask(inp) # 인코더 패딩 마스크\n",
        "  dec_padding_mask = create_padding_mask(inp) # 디코더 패딩 마스크\n",
        "\n",
        "  # 디코더에 대한 마스크는 하삼각행렬을...\n",
        "  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "  dec_target_padding_mask = create_padding_mask(tar)\n",
        "  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "\n",
        "  return enc_padding_mask, combined_mask, dec_padding_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fln-Db3Vc-l0"
      },
      "source": [
        "## positional encoding\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zO1e51nKOgT9"
      },
      "source": [
        "# 위치별 각도를 얻는 함수\n",
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * i / 2) / np.float32(d_model))\n",
        "  return pos * angle_rates"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWwMR39lOgYq"
      },
      "source": [
        "# 최종 포지셔널 인코딩\n",
        "def positional_encoding(position, d_model): # position과 d_model을 parameter로 받음\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "  \n",
        "  # 짝수는 sine 함수를 이용한 pos encoding\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "  # 홀수는 cosine 함수를 이용한 pos encoding\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "  pos_encoding = angle_rads[np.newaixs, ...]\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype = tf.float32) # pos encoding을 실수로 casting"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXlstOwAie8B"
      },
      "source": [
        "## Scaled dot product"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcNwJXqBSv0B"
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "  matmul_qk = tf.matmul(q, k, transpose_b = True) # key에 대해서 matmul 이전 transposed\n",
        "\n",
        "  # scale matmul_qk\n",
        "  dk = tf.cast(tf.shape()[-1], tf.float32)\n",
        "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "  # add!!!!\n",
        "  if mask is not None:\n",
        "    scaled_attention_logits += (mask * -1e9) # 큰 음수를 곱해줘서 mask화 시키도록함....\n",
        "\n",
        "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1) # softmax 적용\n",
        "\n",
        "  output = tf.matmul(attention_weights, v)  # matmul 적용\n",
        "\n",
        "  return output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEjfhFI2ih2Q"
      },
      "source": [
        "## Multihead Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQwrUQM-s2N4"
      },
      "source": [
        "'''\n",
        "1. WQ, WK, WV에 해당하는 d_model 크기의 밀집층(Dense layer)을 지나게한다.\n",
        "2. 지정된 헤드 수(num_heads)만큼 나눈다(split).\n",
        "3. 스케일드 닷 프로덕트 어텐션.\n",
        "4. 나눠졌던 헤드들을 연결(concatenatetion)한다.\n",
        "5. WO에 해당하는 밀집층을 지나게 한다.\n",
        "'''\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kargs):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = kargs['num_heads']\n",
        "        self.d_model = kargs['d_model']\n",
        "\n",
        "        assert self.d_model % self.num_heads == 0\n",
        "\n",
        "        # d_model을 num_heads로 나눈값으로 qkv를 정하기\n",
        "        # 논문에서는 64\n",
        "        self.depth = self.d_model // self.num_heads\n",
        "\n",
        "        # wq wk wv - qkv를 만들기 위한 가중치 행렬\n",
        "        self.wq = tf.keras.layers.Dense(kargs['d_model'])\n",
        "        self.wk = tf.keras.layers.Dense(kargs['d_model'])\n",
        "        self.wv = tf.keras.layers.Dense(kargs['d_model'])\n",
        "\n",
        "        # w0\n",
        "        self.dense = tf.keras.layers.Dense(kargs['d_model'])\n",
        "\n",
        "    # num_heads 개수만큼 q, k, v를 split\n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)  # (batch_size, query_seq_len, d_model)\n",
        "        k = self.wk(k)  # (batch_size, key_seq_len, d_model)\n",
        "        v = self.wv(v)  # (batch_size, value_seq_len, d_model)\n",
        "\n",
        "        # depth = d_moel / num_heads\n",
        "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, query_seq_len_q, depth)\n",
        "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, key_seq_len_k, depth)\n",
        "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, value_seq_len_v, depth)\n",
        "\n",
        "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            q, k, v, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention, \n",
        "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        return output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgRS_WA2imby"
      },
      "source": [
        "## Feed forward network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXONtsgpSvrH"
      },
      "source": [
        "def point_wise_feed_forward_network(**kargs):\n",
        "    return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(kargs['dff'], activation='relu'),  # (batch_size, seq_len, dff:2048)\n",
        "      tf.keras.layers.Dense(kargs['d_model'])  # (batch_size, seq_len, d_model)\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7mr61kuisDG"
      },
      "source": [
        "# Encoder layer\n",
        "(MHA, FFN, norm, dropout)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s319JWynirx-"
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kargs):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(**kargs)\n",
        "        self.ffn = point_wise_feed_forward_network(**kargs)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(kargs['rate'])\n",
        "        self.dropout2 = tf.keras.layers.Dropout(kargs['rate'])\n",
        "\n",
        "    def call(self, x, mask):\n",
        "        '''multihead attention \n",
        "           - dropout\n",
        "           - add & normalization \n",
        "           \n",
        "           - feed forward network \n",
        "           - dropout\n",
        "           - add & normalization'''\n",
        "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "        attn_output = self.dropout1(attn_output)\n",
        "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        return out2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFcOKosViuhU"
      },
      "source": [
        "## Decoder layer\n",
        "(MHA, FFN, norm, dropout)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4dKxvS4jEC8"
      },
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kargs):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(**kargs)\n",
        "        self.mha2 = MultiHeadAttention(**kargs)\n",
        "\n",
        "        self.ffn = point_wise_feed_forward_network(**kargs)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(kargs['rate'])\n",
        "        self.dropout2 = tf.keras.layers.Dropout(kargs['rate'])\n",
        "        self.dropout3 = tf.keras.layers.Dropout(kargs['rate'])\n",
        "    \n",
        "    \n",
        "    def call(self, x, enc_output, look_ahead_mask, padding_mask):\n",
        "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "        '''masked multihead attention \n",
        "           - dropout\n",
        "           - add & normalization \n",
        "           \n",
        "           - multihead attention \n",
        "           - dropout\n",
        "           - add & normalization \n",
        "\n",
        "           - feed forward network \n",
        "           - dropout\n",
        "           - add & normalization'''\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
        "        attn1 = self.dropout1(attn1)\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "        attn2, attn_weights_block2 = self.mha2(\n",
        "            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
        "        attn2 = self.dropout2(attn2)\n",
        "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
        "        ffn_output = self.dropout3(ffn_output)\n",
        "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HK7ofTEdjfrK"
      },
      "source": [
        "## Encoder\n",
        "(Embedding, ENC_layer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvg0hvMriuUd"
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kargs):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.d_model = kargs['d_model']\n",
        "        self.num_layers = kargs['num_layers']\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(kargs['input_vocab_size'], self.d_model)\n",
        "        self.pos_encoding = positional_encoding(kargs['maximum_position_encoding'], \n",
        "                                                self.d_model)\n",
        "\n",
        "\n",
        "        self.enc_layers = [EncoderLayer(**kargs) \n",
        "                           for _ in range(self.num_layers)]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(kargs['rate'])\n",
        "\n",
        "    def call(self, x, mask):\n",
        "\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        '''\n",
        "          embedding\n",
        "          - positional encoding\n",
        "          - dropout\n",
        "          - encoding layers * num_layer(여기서는 2r개의 layer)\n",
        "        '''\n",
        "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, mask)\n",
        "\n",
        "        return x  # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_V0gW1pjpb8"
      },
      "source": [
        "## Decoder\n",
        "(Embedding, DEC_layer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrWT3eG2jp6x"
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kargs):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = kargs['d_model']\n",
        "        self.num_layers = kargs['num_layers']\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(kargs['target_vocab_size'], self.d_model)\n",
        "        self.pos_encoding = positional_encoding(kargs['maximum_position_encoding'], self.d_model)\n",
        "\n",
        "        self.dec_layers = [DecoderLayer(**kargs) \n",
        "                           for _ in range(self.num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(kargs['rate'])\n",
        "\n",
        "    def call(self, x, enc_output, look_ahead_mask, padding_mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "\n",
        "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, look_ahead_mask, padding_mask)\n",
        "\n",
        "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "\n",
        "        # x.shape == (batch_size, target_seq_len, d_model)\n",
        "        return x, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ed7d0S4pkX7t"
      },
      "source": [
        "## Transformer\n",
        "(Encoder, Decoder, final_layer) + inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnUwBcPWkXpV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvAa83rjk9Vq"
      },
      "source": [
        "## loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntJ6tzkakXdO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPQPmpcflBMG"
      },
      "source": [
        "## model compile"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngPHOzBAlFBH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzpTQTCBlFbD"
      },
      "source": [
        "## early stopping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y188WrPWlFyG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}