{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "사전학습모델.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1aGUl5irnIiEviVNGFFlbahNy7YQkqOfC",
      "authorship_tag": "ABX9TyP0UX2dhKZzO+qLFEfQncyM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/silverstar0727/NLP_study/blob/main/%EC%82%AC%EC%A0%84%ED%95%99%EC%8A%B5%EB%AA%A8%EB%8D%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehsLoIt0CGcK",
        "outputId": "90ad5474-7c3b-40cf-85b5-cae84a0b0d3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "cd /content/drive/My Drive/ml/nlp/book/ch7"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/ml/nlp/book/ch7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7Jr-RM3ooKD"
      },
      "source": [
        "## pretrained"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTM_V1AM-yoh"
      },
      "source": [
        "!pip install -q transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nb8uFoQ5zKcz"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import tensorflow as tf\n",
        "from transformers import *\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qk_GpFWv-uIC"
      },
      "source": [
        "# 시각화\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "    plt.plot(history.history[string])\n",
        "    plt.plot(history.history['val_'+string], '')\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(string)\n",
        "    plt.legend([string, 'val_'+string])\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsYwtpvf_E8d"
      },
      "source": [
        "#random seed 고정\n",
        "tf.random.set_seed(1234)\n",
        "np.random.seed(1234)\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 3\n",
        "VALID_SPLIT = 0.2\n",
        "MAX_LEN = 39 # EDA에서 추출된 Max Length\n",
        "DATA_IN_PATH = 'data_in/KOR'\n",
        "DATA_OUT_PATH = \"data_out/KOR\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZThEs6KCCYnd",
        "outputId": "8b56dd25-6536-4feb-ec23-f78f06a2a231",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "# 버트의 다국어 토크나이저를 다운로드\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\", \n",
        "                                          cache_dir='bert_ckpt', do_lower_case=False)\n",
        "\n",
        "#================================\n",
        "# 잘 토크나이즈가 되는지 확인\n",
        "test_sentence = \"안녕하세요, 반갑습니다.\"\n",
        "\n",
        "encode = tokenizer.encode(test_sentence)\n",
        "token_print = [tokenizer.decode(token) for token in encode]\n",
        "\n",
        "print(encode)\n",
        "print(token_print)\n",
        "\n",
        "kor_encode = tokenizer.encode(\"안녕하세요, 반갑습니다\")\n",
        "eng_encode = tokenizer.encode(\"Hello world\")\n",
        "kor_decode = tokenizer.decode(kor_encode)\n",
        "eng_decode = tokenizer.decode(eng_encode)\n",
        "\n",
        "print(kor_encode)\n",
        "# [101, 9521, 118741, 35506, 24982, 48549, 117, 9321, 118610, 119081, 48345, 102]\n",
        "print(eng_encode)\n",
        "# [101, 31178, 11356, 102]\n",
        "print(kor_decode)\n",
        "# [CLS] 안녕하세요, 반갑습니다 [SEP]\n",
        "print(eng_decode)\n",
        "# [CLS] Hello world [SEP]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[101, 9521, 118741, 35506, 24982, 48549, 117, 9321, 118610, 119081, 48345, 119, 102]\n",
            "['[ C L S ]', '안', '# # 녕', '# # 하', '# # 세', '# # 요', ',', '반', '# # 갑', '# # 습', '# # 니 다', '.', '[ S E P ]']\n",
            "[101, 9521, 118741, 35506, 24982, 48549, 117, 9321, 118610, 119081, 48345, 102]\n",
            "[101, 31178, 11356, 102]\n",
            "[CLS] 안녕하세요, 반갑습니다 [SEP]\n",
            "[CLS] Hello world [SEP]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJJNNajjjnNQ"
      },
      "source": [
        "## 네이버 영화리뷰 데이터 전처리\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YomM2ze0CYsc",
        "outputId": "2154e9d7-81b3-4235-c14e-4759572e0e99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# 데이터 전처리 준비\n",
        "DATA_TRAIN_PATH = os.path.join(DATA_IN_PATH, \"naver_movie\", \"ratings_train.txt\")\n",
        "DATA_TEST_PATH = os.path.join(DATA_IN_PATH, \"naver_movie\", \"ratings_test.txt\")\n",
        "\n",
        "train_data = pd.read_csv(DATA_TRAIN_PATH, header = 0, delimiter = '\\t', quoting = 3)\n",
        "train_data = train_data.dropna()\n",
        "train_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>document</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9976970</td>\n",
              "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3819312</td>\n",
              "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10265843</td>\n",
              "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9045019</td>\n",
              "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6483659</td>\n",
              "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         id                                           document  label\n",
              "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
              "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
              "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
              "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
              "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgg0LrvVjzKy",
        "outputId": "e24c5835-3226-44b7-d1ce-a16d78fecdf3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "# 스페셜 토큰\n",
        "print(tokenizer.all_special_tokens, \"\\n\", tokenizer.all_special_ids)\n",
        "\n",
        "# 토크나이저 테스트하기\n",
        "kor_encode = tokenizer.encode(\"안녕하세요, 반갑습니다. \")\n",
        "eng_encode = tokenizer.encode(\"Hello world\")\n",
        "\n",
        "kor_decode = tokenizer.decode(kor_encode)\n",
        "eng_decode = tokenizer.decode(eng_encode)\n",
        "\n",
        "print(kor_encode)\n",
        "print(eng_encode)\n",
        "print(kor_decode)\n",
        "print(eng_decode)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]'] \n",
            " [100, 102, 0, 101, 103]\n",
            "[101, 9521, 118741, 35506, 24982, 48549, 117, 9321, 118610, 119081, 48345, 119, 102]\n",
            "[101, 31178, 11356, 102]\n",
            "[CLS] 안녕하세요, 반갑습니다. [SEP]\n",
            "[CLS] Hello world [SEP]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Onf8m4yWjtW_"
      },
      "source": [
        "## hugging face Bert Tokenizer\n",
        "허깅페이스의 Tokenizer 라이브러리를 이용한 전처리\n",
        "\n",
        "encode_plus기능을 활용하여 토크나이징 진행 \n",
        "\n",
        "cpu에 최적화 됨 -> 20초내에 GB단위의 문장을 토크나이징 가능\n",
        "\n",
        "* encode_plus의 변환 순서\n",
        " * 문장을 토크나이징\n",
        " * add_special_token = True 를 통해 ['CLS'], ['SEP']토큰을 붙임\n",
        " * 토큰을 인덱스로 변환\n",
        " * max_length = MAX_LANGTH 로 최대길이에 따라 문장의 길이를 맞춤\n",
        " * pad_to_max_length = Ture 로 최대길이보다 짤은 것들을 패딩시킴\n",
        " * return_attention_mask = Ture 로 어텐션 마스크를 생성 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gxiAyOhCYwn"
      },
      "source": [
        "# Bert Tokenizer\n",
        "\n",
        "# 참조: https://huggingface.co/transformers/main_classes/tokenizer.html?highlight=encode_plus#transformers.PreTrainedTokenizer.encode_plus\n",
        "\n",
        "def bert_tokenizer(sent, MAX_LEN):\n",
        "    \n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "        text = sent,\n",
        "        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "        max_length = MAX_LEN,           # Pad & truncate all sentences.\n",
        "        pad_to_max_length = True,\n",
        "        return_attention_mask = True   # Construct attn. masks.\n",
        "        \n",
        "    )\n",
        "    \n",
        "    input_id = encoded_dict['input_ids']\n",
        "    attention_mask = encoded_dict['attention_mask'] # And its attention mask (simply differentiates padding from non-padding).\n",
        "    token_type_id = encoded_dict['token_type_ids'] # differentiate two sentences\n",
        "    \n",
        "    return input_id, attention_mask, token_type_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxnMBOGS3gnH"
      },
      "source": [
        "## 네이버 영화 리뷰 전처리\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7ryLE38CYuo",
        "outputId": "52c50867-13c9-4a81-8581-bde9919ba60a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "# train_data = train_data[:1000] # for test\n",
        "\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "token_type_ids = []\n",
        "train_data_labels = []\n",
        "\n",
        "for train_sent, train_label in tqdm(zip(train_data[\"document\"], train_data[\"label\"]), total=len(train_data)):\n",
        "    try:\n",
        "        input_id, attention_mask, token_type_id = bert_tokenizer(train_sent, MAX_LEN)\n",
        "        \n",
        "        input_ids.append(input_id)\n",
        "        attention_masks.append(attention_mask)\n",
        "        token_type_ids.append(token_type_id)\n",
        "        train_data_labels.append(train_label)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print(train_sent)\n",
        "        pass\n",
        "\n",
        "train_movie_input_ids = np.array(input_ids, dtype=int)\n",
        "train_movie_attention_masks = np.array(attention_masks, dtype=int)\n",
        "train_movie_type_ids = np.array(token_type_ids, dtype=int)\n",
        "train_movie_inputs = (train_movie_input_ids, train_movie_attention_masks, train_movie_type_ids)\n",
        "\n",
        "train_data_labels = np.asarray(train_data_labels, dtype=np.int32) #레이블 토크나이징 리스트\n",
        "\n",
        "print(\"# sents: {}, # labels: {}\".format(len(train_movie_input_ids), len(train_data_labels)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/149995 [00:00<?, ?it/s]Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "100%|██████████| 149995/149995 [00:46<00:00, 3209.90it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "# sents: 149995, # labels: 149995\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FJ11d2sCYp1",
        "outputId": "1bf97d37-7b99-4a3a-eb5d-56c1b5c31762",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "source": [
        "# 최대 길이: 39\n",
        "input_id = train_movie_input_ids[1]\n",
        "attention_mask = train_movie_attention_masks[1]\n",
        "token_type_id = train_movie_type_ids[1]\n",
        "\n",
        "print(input_id)\n",
        "print(attention_mask)\n",
        "print(token_type_id)\n",
        "print(tokenizer.decode(input_id))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[   101    100    119    119    119   9928  58823  30005  11664   9757\n",
            " 118823  30858  18227 119219    119    119    119    119   9580  41605\n",
            "  25486  12310  20626  23466   8843 118986  12508   9523  17196  16439\n",
            "    102      0      0      0      0      0      0      0      0]\n",
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0\n",
            " 0 0]\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0]\n",
            "[CLS] [UNK]... 포스터보고 초딩영화줄.... 오버연기조차 가볍지 않구나 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-E69wi0bCYlI"
      },
      "source": [
        "# BERT를 이용한 classifier modeling\n",
        "class TFBertClassifier(tf.keras.Model):\n",
        "    def __init__(self, model_name, dir_path, num_class):\n",
        "        super(TFBertClassifier, self).__init__()\n",
        "\n",
        "        self.bert = TFBertModel.from_pretrained(model_name, cache_dir=dir_path) # 사전학습된 결과 불러오기\n",
        "        self.dropout = tf.keras.layers.Dropout(self.bert.config.hidden_dropout_prob) # dropout\n",
        "        self.classifier = tf.keras.layers.Dense(num_class, \n",
        "                                                kernel_initializer = tf.keras.initializers.TruncatedNormal(self.bert.config.initializer_range), \n",
        "                                                name=\"classifier\")\n",
        "        \n",
        "    def call(self, inputs, attention_mask=None, token_type_ids=None, training=False):\n",
        "        \n",
        "        #outputs 값: # sequence_output, pooled_output, (hidden_states), (attentions)\n",
        "        outputs = self.bert(inputs, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        pooled_output = outputs[1] \n",
        "        pooled_output = self.dropout(pooled_output, training=training)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        return logits\n",
        "\n",
        "cls_model = TFBertClassifier(model_name='bert-base-multilingual-cased',\n",
        "                                  dir_path='bert_ckpt',\n",
        "                                  num_class=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPFLQBIlj6bs"
      },
      "source": [
        "# 학습 준비하기\n",
        "optimizer = tf.keras.optimizers.Adam(3e-5)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "cls_model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dZ6_Kzwj6gC"
      },
      "source": [
        "model_name = \"tf2_bert_naver_movie\"\n",
        "\n",
        "# overfitting을 막기 위한 ealrystop 추가\n",
        "earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001,patience=2)\n",
        "# min_delta: the threshold that triggers the termination (acc should at least improve 0.0001)\n",
        "# patience: no improvment epochs (patience = 1, 1번 이상 상승이 없으면 종료)\\\n",
        "\n",
        "checkpoint_path = os.path.join(DATA_OUT_PATH, model_name, 'weights.h5')\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Create path if exists\n",
        "if os.path.exists(checkpoint_dir):\n",
        "    print(\"{} -- Folder already exists \\n\".format(checkpoint_dir))\n",
        "else:\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    print(\"{} -- Folder create complete \\n\".format(checkpoint_dir))\n",
        "    \n",
        "cp_callback = ModelCheckpoint(\n",
        "    checkpoint_path, monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=True)\n",
        "\n",
        "# 학습과 eval 시작\n",
        "history = cls_model.fit(train_movie_inputs, train_data_labels, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE,\n",
        "                    validation_split = VALID_SPLIT, callbacks=[earlystop_callback, cp_callback])\n",
        "\n",
        "#steps_for_epoch\n",
        "\n",
        "print(history.history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KN7jwvMNj6eH"
      },
      "source": [
        "test_data = pd.read_csv(DATA_TEST_PATH, header = 0, delimiter = '\\t', quoting = 3)\n",
        "test_data = test_data.dropna()\n",
        "test_data.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wy3HxfhMj6Y4"
      },
      "source": [
        "input_ids = []\n",
        "attention_masks = []\n",
        "token_type_ids = []\n",
        "test_data_labels = []\n",
        "\n",
        "for test_sent, test_label in tqdm(zip(test_data[\"document\"], test_data[\"label\"])):\n",
        "    try:\n",
        "        input_id, attention_mask, token_type_id = bert_tokenizer(test_sent, MAX_LEN)\n",
        "\n",
        "        input_ids.append(input_id)\n",
        "        attention_masks.append(attention_mask)\n",
        "        token_type_ids.append(token_type_id)\n",
        "        test_data_labels.append(test_label)\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print(test_sent)\n",
        "        pass\n",
        "\n",
        "test_movie_input_ids = np.array(input_ids, dtype=int)\n",
        "test_movie_attention_masks = np.array(attention_masks, dtype=int)\n",
        "test_movie_type_ids = np.array(token_type_ids, dtype=int)\n",
        "test_movie_inputs = (test_movie_input_ids, test_movie_attention_masks, test_movie_type_ids)\n",
        "\n",
        "test_data_labels = np.asarray(test_data_labels, dtype=np.int32) #레이블 토크나이징 리스트\n",
        "\n",
        "print(\"num sents, labels {}, {}\".format(len(test_movie_input_ids), len(test_data_labels)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmX1mGz8j_Ar"
      },
      "source": [
        "results = cls_model.evaluate(test_movie_inputs, test_data_labels, batch_size=1024)\n",
        "print(\"test loss, test acc: \", results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOXBOa0uY_Ud"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}