{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch6_transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "15gllnY1v_DoZNYnaHh4DpRA8OEPB3fl9",
      "authorship_tag": "ABX9TyPvGqH7davukxhOzA1uTLHY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/silverstar0727/NLP_study/blob/main/ch6_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExzFv22mj_6v",
        "outputId": "71784aa4-2bbc-4317-9283-ceb95b71c9f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content/drive/My Drive/ml/nlp/book/ch6"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/ml/nlp/book/ch6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2r0F3jaxkH0U"
      },
      "source": [
        "!pip install -q konlpy\n",
        "!pip install -q preprocessing"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKAmHqAAjyhN"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from konlpy.tag import Twitter\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import enum\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from preprocess import *"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcj6_uszjzMh"
      },
      "source": [
        "def plot_graphs(history, string):\n",
        "    plt.plot(history.history[string])\n",
        "    plt.plot(history.history['val_'+string], '')\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(string)\n",
        "    plt.legend([string, 'val_'+string])\n",
        "    plt.show()"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehfZgpoPjzKY"
      },
      "source": [
        "DATA_OUT_PATH = 'output/'\n",
        "TRAIN_INPUTS = 'train_inputs.npy'\n",
        "TRAIN_OUTPUTS = 'train_outputs.npy'\n",
        "TRAIN_TARGETS = 'train_targets.npy'\n",
        "DATA_CONFIGS = 'data_configs.json'"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6XOB8WSjzEn"
      },
      "source": [
        "SEED_NUM = 1234\n",
        "tf.random.set_seed(SEED_NUM)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AZX5BpfkNzz"
      },
      "source": [
        "index_inputs = np.load(open(TRAIN_INPUTS, 'rb'))\n",
        "index_outputs = np.load(open(TRAIN_OUTPUTS , 'rb'))\n",
        "index_targets = np.load(open(TRAIN_TARGETS , 'rb'))\n",
        "prepro_configs = json.load(open(DATA_CONFIGS, 'r'))"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8ExmbmTkN5K"
      },
      "source": [
        "char2idx = prepro_configs['char2idx']\n",
        "end_index = prepro_configs['end_symbol']\n",
        "model_name = 'transformer'\n",
        "vocab_size = prepro_configs['vocab_size']\n",
        "BATCH_SIZE = 64\n",
        "MAX_SEQUENCE = 25\n",
        "EPOCHS = 35\n",
        "VALID_SPLIT = 0.1\n",
        "\n",
        "kargs = {'model_name': model_name,\n",
        "         'num_layers': 2,\n",
        "         'd_model': 512,\n",
        "         'num_heads': 8,\n",
        "         'dff': 2048,\n",
        "         'input_vocab_size': vocab_size,\n",
        "         'target_vocab_size': vocab_size,\n",
        "         'maximum_position_encoding': MAX_SEQUENCE,\n",
        "         'end_token_idx': char2idx[end_index],\n",
        "         'rate': 0.1\n",
        "        }"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruDqfoizkN9j"
      },
      "source": [
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "\n",
        "    # add extra dimensions to add the padding\n",
        "    # to the attention logits.\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgvVGQtxkN7Y"
      },
      "source": [
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask  # (seq_len, seq_len)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXMfynVnkN2Q"
      },
      "source": [
        "def create_masks(inp, tar):\n",
        "    # Encoder padding mask\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    # Used in the 2nd attention block in the decoder.\n",
        "    # This padding mask is used to mask the encoder outputs.\n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    # Used in the 1st attention block in the decoder.\n",
        "    # It is used to pad and mask future tokens in the input received by \n",
        "    # the decoder.\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "\n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5g6XSWE4kNeA"
      },
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * i//2) / np.float32(d_model))\n",
        "    return pos * angle_rates"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Boox9rkwkNb3"
      },
      "source": [
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "\n",
        "    # apply sin to even indices in the array; 2i\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "    # apply cos to odd indices in the array; 2i+1\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dq6ZwWuDkeJP",
        "outputId": "e47fcec3-4664-4177-ce12-8bd05dd96006",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "pos_encoding = positional_encoding(50, 512)\n",
        "print (pos_encoding.shape)\n",
        "\n",
        "plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n",
        "plt.xlabel('Depth')\n",
        "plt.xlim((0, 512))\n",
        "plt.ylabel('Position')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 50, 512)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poGmKpbOkeNP"
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    \"\"\"Calculate the attention weights.\n",
        "    q, k, v must have matching leading dimensions.\n",
        "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "    The mask has different shapes depending on its type(padding or look ahead) \n",
        "    but it must be broadcastable for addition.\n",
        "\n",
        "    Args:\n",
        "    q: query shape == (..., seq_len_q, depth)\n",
        "    k: key shape == (..., seq_len_k, depth)\n",
        "    v: value shape == (..., seq_len_v, depth_v)\n",
        "    mask: Float tensor with shape broadcastable \n",
        "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "    output, attention_weights\n",
        "    \"\"\"\n",
        "\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    # scale matmul_qk\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    # add the mask to the scaled tensor.\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)  \n",
        "\n",
        "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "    # add up to 1.\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "    return output, attention_weights"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jM9r7MJBkeR2"
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kargs):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = kargs['num_heads']\n",
        "        self.d_model = kargs['d_model']\n",
        "\n",
        "        assert self.d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = self.d_model // self.num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(kargs['d_model'])\n",
        "        self.wk = tf.keras.layers.Dense(kargs['d_model'])\n",
        "        self.wv = tf.keras.layers.Dense(kargs['d_model'])\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(kargs['d_model'])\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"Split the last dimension into (num_heads, depth).\n",
        "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            q, k, v, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention, \n",
        "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        return output, attention_weights"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivHNBtQHkePU"
      },
      "source": [
        "def point_wise_feed_forward_network(**kargs):\n",
        "    return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(kargs['dff'], activation='relu'),  # (batch_size, seq_len, dff)\n",
        "      tf.keras.layers.Dense(kargs['d_model'])  # (batch_size, seq_len, d_model)\n",
        "    ])"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHvJiIPlkeLX"
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kargs):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(**kargs)\n",
        "        self.ffn = point_wise_feed_forward_network(**kargs)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(kargs['rate'])\n",
        "        self.dropout2 = tf.keras.layers.Dropout(kargs['rate'])\n",
        "\n",
        "    def call(self, x, mask):\n",
        "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "        attn_output = self.dropout1(attn_output)\n",
        "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        return out2"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-QFVU-mkeGy"
      },
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kargs):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(**kargs)\n",
        "        self.mha2 = MultiHeadAttention(**kargs)\n",
        "\n",
        "        self.ffn = point_wise_feed_forward_network(**kargs)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(kargs['rate'])\n",
        "        self.dropout2 = tf.keras.layers.Dropout(kargs['rate'])\n",
        "        self.dropout3 = tf.keras.layers.Dropout(kargs['rate'])\n",
        "    \n",
        "    \n",
        "    def call(self, x, enc_output, look_ahead_mask, padding_mask):\n",
        "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
        "        attn1 = self.dropout1(attn1)\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "        attn2, attn_weights_block2 = self.mha2(\n",
        "            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
        "        attn2 = self.dropout2(attn2)\n",
        "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
        "        ffn_output = self.dropout3(ffn_output)\n",
        "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvqmdYDekoCe"
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kargs):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.d_model = kargs['d_model']\n",
        "        self.num_layers = kargs['num_layers']\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(kargs['input_vocab_size'], self.d_model)\n",
        "        self.pos_encoding = positional_encoding(kargs['maximum_position_encoding'], \n",
        "                                                self.d_model)\n",
        "\n",
        "\n",
        "        self.enc_layers = [EncoderLayer(**kargs) \n",
        "                           for _ in range(self.num_layers)]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(kargs['rate'])\n",
        "\n",
        "    def call(self, x, mask):\n",
        "\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        # adding embedding and position encoding.\n",
        "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, mask)\n",
        "\n",
        "        return x  # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gsif9AYAkoEl"
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kargs):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = kargs['d_model']\n",
        "        self.num_layers = kargs['num_layers']\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(kargs['target_vocab_size'], self.d_model)\n",
        "        self.pos_encoding = positional_encoding(kargs['maximum_position_encoding'], self.d_model)\n",
        "\n",
        "        self.dec_layers = [DecoderLayer(**kargs) \n",
        "                           for _ in range(self.num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(kargs['rate'])\n",
        "\n",
        "    def call(self, x, enc_output, look_ahead_mask, padding_mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "\n",
        "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, look_ahead_mask, padding_mask)\n",
        "\n",
        "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "\n",
        "        # x.shape == (batch_size, target_seq_len, d_model)\n",
        "        return x, attention_weights"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rm0R1-B_koHN"
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, **kargs):\n",
        "        super(Transformer, self).__init__(name=kargs['model_name'])\n",
        "        self.end_token_idx = kargs['end_token_idx']\n",
        "        \n",
        "        self.encoder = Encoder(**kargs)\n",
        "        self.decoder = Decoder(**kargs)\n",
        "\n",
        "        self.final_layer = tf.keras.layers.Dense(kargs['target_vocab_size'])\n",
        "\n",
        "    def call(self, x):\n",
        "        inp, tar = x\n",
        "\n",
        "        enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(inp, tar)\n",
        "        enc_output = self.encoder(inp, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
        "\n",
        "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "        dec_output, _ = self.decoder(\n",
        "            tar, enc_output, look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "\n",
        "        return final_output\n",
        "    \n",
        "    def inference(self, x):\n",
        "        inp = x\n",
        "        tar = tf.expand_dims([STD_INDEX], 0)\n",
        "\n",
        "        enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(inp, tar)        \n",
        "        enc_output = self.encoder(inp, enc_padding_mask)\n",
        "        \n",
        "        predict_tokens = list()\n",
        "        for t in range(0, MAX_SEQUENCE):\n",
        "            dec_output, _ = self.decoder(tar, enc_output, look_ahead_mask, dec_padding_mask)\n",
        "            final_output = self.final_layer(dec_output)\n",
        "            outputs = tf.argmax(final_output, -1).numpy()\n",
        "            pred_token = outputs[0][-1]\n",
        "            if pred_token == self.end_token_idx:\n",
        "                break\n",
        "            predict_tokens.append(pred_token)\n",
        "            tar = tf.expand_dims([STD_INDEX] + predict_tokens, 0)\n",
        "            _, look_ahead_mask, dec_padding_mask = create_masks(inp, tar)\n",
        "            \n",
        "        return predict_tokens"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nVnrm6ukn_h"
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')\n",
        "\n",
        "def loss(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)\n",
        "\n",
        "def accuracy(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    mask = tf.expand_dims(tf.cast(mask, dtype=pred.dtype), axis=-1)\n",
        "    pred *= mask    \n",
        "    acc = train_accuracy(real, pred)\n",
        "\n",
        "    return tf.reduce_mean(acc)"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceZGoMijkvCj"
      },
      "source": [
        "model = Transformer(**kargs)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              loss=loss,\n",
        "              metrics=[accuracy])"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zB_0rH9kvHP",
        "outputId": "88b69d1f-3679-4547-82e0-3a49909a8a76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# overfitting   ealrystop \n",
        "earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=10)\n",
        "# min_delta: the threshold that triggers the termination (acc should at least improve 0.0001)\n",
        "# patience: no improvment epochs (patience = 1, 1    )\n",
        "\n",
        "checkpoint_path = DATA_OUT_PATH + model_name + '/weights.h5'\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Create path if exists\n",
        "if os.path.exists(checkpoint_dir):\n",
        "    print(\"{} -- Folder already exists \\n\".format(checkpoint_dir))\n",
        "else:\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    print(\"{} -- Folder create complete \\n\".format(checkpoint_dir))\n",
        "    \n",
        "\n",
        "cp_callback = ModelCheckpoint(\n",
        "    checkpoint_path, monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=True)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "output/transformer -- Folder already exists \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7in2Y623kvAA",
        "outputId": "ec2b1efb-054c-4a93-f691-e79e461680f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit([index_inputs, index_outputs], index_targets, \n",
        "                    batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
        "                    validation_split=VALID_SPLIT, callbacks=[earlystop_callback, cp_callback])"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/35\n",
            "167/167 [==============================] - ETA: 0s - loss: 1.6104 - accuracy: 0.8098\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.80772, saving model to output/transformer/weights.h5\n",
            "167/167 [==============================] - 38s 225ms/step - loss: 1.6104 - accuracy: 0.8098 - val_loss: 1.6829 - val_accuracy: 0.8077\n",
            "Epoch 2/35\n",
            "167/167 [==============================] - ETA: 0s - loss: 1.3506 - accuracy: 0.8093\n",
            "Epoch 00002: val_accuracy improved from 0.80772 to 0.81101, saving model to output/transformer/weights.h5\n",
            "167/167 [==============================] - 37s 221ms/step - loss: 1.3506 - accuracy: 0.8093 - val_loss: 1.5963 - val_accuracy: 0.8110\n",
            "Epoch 3/35\n",
            "167/167 [==============================] - ETA: 0s - loss: 1.2217 - accuracy: 0.8139\n",
            "Epoch 00003: val_accuracy improved from 0.81101 to 0.81654, saving model to output/transformer/weights.h5\n",
            "167/167 [==============================] - 37s 220ms/step - loss: 1.2217 - accuracy: 0.8139 - val_loss: 1.5318 - val_accuracy: 0.8165\n",
            "Epoch 4/35\n",
            "167/167 [==============================] - ETA: 0s - loss: 1.1104 - accuracy: 0.8193\n",
            "Epoch 00004: val_accuracy improved from 0.81654 to 0.82188, saving model to output/transformer/weights.h5\n",
            "167/167 [==============================] - 37s 220ms/step - loss: 1.1104 - accuracy: 0.8193 - val_loss: 1.5025 - val_accuracy: 0.8219\n",
            "Epoch 5/35\n",
            "167/167 [==============================] - ETA: 0s - loss: 1.0098 - accuracy: 0.8246\n",
            "Epoch 00005: val_accuracy improved from 0.82188 to 0.82699, saving model to output/transformer/weights.h5\n",
            "167/167 [==============================] - 37s 219ms/step - loss: 1.0098 - accuracy: 0.8246 - val_loss: 1.4826 - val_accuracy: 0.8270\n",
            "Epoch 6/35\n",
            "167/167 [==============================] - ETA: 0s - loss: 0.9161 - accuracy: 0.8297\n",
            "Epoch 00006: val_accuracy improved from 0.82699 to 0.83200, saving model to output/transformer/weights.h5\n",
            "167/167 [==============================] - 37s 220ms/step - loss: 0.9161 - accuracy: 0.8297 - val_loss: 1.4740 - val_accuracy: 0.8320\n",
            "Epoch 7/35\n",
            "167/167 [==============================] - ETA: 0s - loss: 0.8250 - accuracy: 0.8345\n",
            "Epoch 00007: val_accuracy improved from 0.83200 to 0.83695, saving model to output/transformer/weights.h5\n",
            "167/167 [==============================] - 37s 221ms/step - loss: 0.8250 - accuracy: 0.8345 - val_loss: 1.4852 - val_accuracy: 0.8370\n",
            "Epoch 8/35\n",
            "167/167 [==============================] - ETA: 0s - loss: 0.7376 - accuracy: 0.8396\n",
            "Epoch 00008: val_accuracy improved from 0.83695 to 0.84188, saving model to output/transformer/weights.h5\n",
            "167/167 [==============================] - 37s 221ms/step - loss: 0.7376 - accuracy: 0.8396 - val_loss: 1.4881 - val_accuracy: 0.8419\n",
            "Epoch 9/35\n",
            "167/167 [==============================] - ETA: 0s - loss: 0.6549 - accuracy: 0.8445\n",
            "Epoch 00009: val_accuracy improved from 0.84188 to 0.84685, saving model to output/transformer/weights.h5\n",
            "167/167 [==============================] - 37s 222ms/step - loss: 0.6549 - accuracy: 0.8445 - val_loss: 1.5003 - val_accuracy: 0.8468\n",
            "Epoch 10/35\n",
            "167/167 [==============================] - ETA: 0s - loss: 0.5763 - accuracy: 0.8495\n",
            "Epoch 00010: val_accuracy improved from 0.84685 to 0.85188, saving model to output/transformer/weights.h5\n",
            "167/167 [==============================] - 37s 220ms/step - loss: 0.5763 - accuracy: 0.8495 - val_loss: 1.5115 - val_accuracy: 0.8519\n",
            "Epoch 11/35\n",
            "167/167 [==============================] - ETA: 0s - loss: 0.5033 - accuracy: 0.8545\n",
            "Epoch 00011: val_accuracy improved from 0.85188 to 0.85691, saving model to output/transformer/weights.h5\n",
            "167/167 [==============================] - 37s 220ms/step - loss: 0.5033 - accuracy: 0.8545 - val_loss: 1.5199 - val_accuracy: 0.8569\n",
            "Epoch 12/35\n",
            "167/167 [==============================] - ETA: 0s - loss: 0.4352 - accuracy: 0.8596\n",
            "Epoch 00012: val_accuracy improved from 0.85691 to 0.86200, saving model to output/transformer/weights.h5\n",
            "167/167 [==============================] - 37s 220ms/step - loss: 0.4352 - accuracy: 0.8596 - val_loss: 1.5518 - val_accuracy: 0.8620\n",
            "Epoch 13/35\n",
            "167/167 [==============================] - ETA: 0s - loss: 0.3735 - accuracy: 0.8646\n",
            "Epoch 00013: val_accuracy improved from 0.86200 to 0.86706, saving model to output/transformer/weights.h5\n",
            "167/167 [==============================] - 37s 221ms/step - loss: 0.3735 - accuracy: 0.8646 - val_loss: 1.5616 - val_accuracy: 0.8671\n",
            "Epoch 14/35\n",
            "167/167 [==============================] - ETA: 0s - loss: 0.3183 - accuracy: 0.8697\n",
            "Epoch 00014: val_accuracy improved from 0.86706 to 0.87205, saving model to output/transformer/weights.h5\n",
            "167/167 [==============================] - 37s 220ms/step - loss: 0.3183 - accuracy: 0.8697 - val_loss: 1.5813 - val_accuracy: 0.8720\n",
            "Epoch 15/35\n",
            "167/167 [==============================] - ETA: 0s - loss: 0.2678 - accuracy: 0.8746\n",
            "Epoch 00015: val_accuracy improved from 0.87205 to 0.87693, saving model to output/transformer/weights.h5\n",
            "167/167 [==============================] - 37s 222ms/step - loss: 0.2678 - accuracy: 0.8746 - val_loss: 1.5993 - val_accuracy: 0.8769\n",
            "Epoch 16/35\n",
            "167/167 [==============================] - ETA: 0s - loss: 0.2234 - accuracy: 0.8794\n",
            "Epoch 00016: val_accuracy improved from 0.87693 to 0.88165, saving model to output/transformer/weights.h5\n",
            "167/167 [==============================] - 37s 220ms/step - loss: 0.2234 - accuracy: 0.8794 - val_loss: 1.6157 - val_accuracy: 0.8817\n",
            "Epoch 17/35\n",
            "167/167 [==============================] - ETA: 0s - loss: 0.1857 - accuracy: 0.8840\n",
            "Epoch 00017: val_accuracy improved from 0.88165 to 0.88620, saving model to output/transformer/weights.h5\n",
            "167/167 [==============================] - 37s 222ms/step - loss: 0.1857 - accuracy: 0.8840 - val_loss: 1.6351 - val_accuracy: 0.8862\n",
            "Epoch 18/35\n",
            "167/167 [==============================] - ETA: 0s - loss: 0.1534 - accuracy: 0.8885\n",
            "Epoch 00018: val_accuracy improved from 0.88620 to 0.89052, saving model to output/transformer/weights.h5\n",
            "167/167 [==============================] - 37s 219ms/step - loss: 0.1534 - accuracy: 0.8885 - val_loss: 1.6555 - val_accuracy: 0.8905\n",
            "Epoch 19/35\n",
            "167/167 [==============================] - ETA: 0s - loss: 0.1263 - accuracy: 0.8926\n",
            "Epoch 00019: val_accuracy improved from 0.89052 to 0.89461, saving model to output/transformer/weights.h5\n",
            "167/167 [==============================] - 37s 219ms/step - loss: 0.1263 - accuracy: 0.8926 - val_loss: 1.6615 - val_accuracy: 0.8946\n",
            "Epoch 20/35\n",
            "167/167 [==============================] - ETA: 0s - loss: 0.1024 - accuracy: 0.8966\n",
            "Epoch 00020: val_accuracy improved from 0.89461 to 0.89848, saving model to output/transformer/weights.h5\n",
            "167/167 [==============================] - 36s 218ms/step - loss: 0.1024 - accuracy: 0.8966 - val_loss: 1.6921 - val_accuracy: 0.8985\n",
            "Epoch 21/35\n",
            "167/167 [==============================] - ETA: 0s - loss: 0.0827 - accuracy: 0.9004\n",
            "Epoch 00021: val_accuracy improved from 0.89848 to 0.90213, saving model to output/transformer/weights.h5\n",
            "167/167 [==============================] - 37s 219ms/step - loss: 0.0827 - accuracy: 0.9004 - val_loss: 1.7034 - val_accuracy: 0.9021\n",
            "Epoch 22/35\n",
            "167/167 [==============================] - ETA: 0s - loss: 0.0677 - accuracy: 0.9039\n",
            "Epoch 00022: val_accuracy improved from 0.90213 to 0.90551, saving model to output/transformer/weights.h5\n",
            "167/167 [==============================] - 37s 221ms/step - loss: 0.0677 - accuracy: 0.9039 - val_loss: 1.7272 - val_accuracy: 0.9055\n",
            "Epoch 23/35\n",
            "167/167 [==============================] - ETA: 0s - loss: 0.0556 - accuracy: 0.9071\n",
            "Epoch 00023: val_accuracy improved from 0.90551 to 0.90866, saving model to output/transformer/weights.h5\n",
            "167/167 [==============================] - 37s 219ms/step - loss: 0.0556 - accuracy: 0.9071 - val_loss: 1.7335 - val_accuracy: 0.9087\n",
            "Epoch 24/35\n",
            "167/167 [==============================] - ETA: 0s - loss: 0.0451 - accuracy: 0.9102\n",
            "Epoch 00024: val_accuracy improved from 0.90866 to 0.91158, saving model to output/transformer/weights.h5\n",
            "167/167 [==============================] - 37s 221ms/step - loss: 0.0451 - accuracy: 0.9102 - val_loss: 1.7537 - val_accuracy: 0.9116\n",
            "Epoch 25/35\n",
            "167/167 [==============================] - ETA: 0s - loss: 0.0366 - accuracy: 0.9130\n",
            "Epoch 00025: val_accuracy improved from 0.91158 to 0.91430, saving model to output/transformer/weights.h5\n",
            "167/167 [==============================] - 37s 219ms/step - loss: 0.0366 - accuracy: 0.9130 - val_loss: 1.7702 - val_accuracy: 0.9143\n",
            "Epoch 26/35\n",
            "167/167 [==============================] - ETA: 0s - loss: 0.0306 - accuracy: 0.9156\n",
            "Epoch 00026: val_accuracy improved from 0.91430 to 0.91682, saving model to output/transformer/weights.h5\n",
            "167/167 [==============================] - 37s 219ms/step - loss: 0.0306 - accuracy: 0.9156 - val_loss: 1.7882 - val_accuracy: 0.9168\n",
            "Epoch 27/35\n",
            "167/167 [==============================] - ETA: 0s - loss: 0.0257 - accuracy: 0.9180\n",
            "Epoch 00027: val_accuracy improved from 0.91682 to 0.91916, saving model to output/transformer/weights.h5\n",
            "167/167 [==============================] - 37s 220ms/step - loss: 0.0257 - accuracy: 0.9180 - val_loss: 1.7998 - val_accuracy: 0.9192\n",
            "Epoch 28/35\n",
            "167/167 [==============================] - ETA: 0s - loss: 0.0222 - accuracy: 0.9203\n",
            "Epoch 00028: val_accuracy improved from 0.91916 to 0.92134, saving model to output/transformer/weights.h5\n",
            "167/167 [==============================] - 37s 220ms/step - loss: 0.0222 - accuracy: 0.9203 - val_loss: 1.8190 - val_accuracy: 0.9213\n",
            "Epoch 29/35\n",
            "167/167 [==============================] - ETA: 0s - loss: 0.0193 - accuracy: 0.9224\n",
            "Epoch 00029: val_accuracy improved from 0.92134 to 0.92337, saving model to output/transformer/weights.h5\n",
            "167/167 [==============================] - 37s 219ms/step - loss: 0.0193 - accuracy: 0.9224 - val_loss: 1.8236 - val_accuracy: 0.9234\n",
            "Epoch 30/35\n",
            "167/167 [==============================] - ETA: 0s - loss: 0.0172 - accuracy: 0.9244\n",
            "Epoch 00030: val_accuracy improved from 0.92337 to 0.92527, saving model to output/transformer/weights.h5\n",
            "167/167 [==============================] - 37s 219ms/step - loss: 0.0172 - accuracy: 0.9244 - val_loss: 1.8330 - val_accuracy: 0.9253\n",
            "Epoch 31/35\n",
            "167/167 [==============================] - ETA: 0s - loss: 0.0154 - accuracy: 0.9262\n",
            "Epoch 00031: val_accuracy improved from 0.92527 to 0.92705, saving model to output/transformer/weights.h5\n",
            "167/167 [==============================] - 36s 218ms/step - loss: 0.0154 - accuracy: 0.9262 - val_loss: 1.8475 - val_accuracy: 0.9271\n",
            "Epoch 32/35\n",
            "167/167 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 0.9279\n",
            "Epoch 00032: val_accuracy improved from 0.92705 to 0.92872, saving model to output/transformer/weights.h5\n",
            "167/167 [==============================] - 36s 217ms/step - loss: 0.0140 - accuracy: 0.9279 - val_loss: 1.8599 - val_accuracy: 0.9287\n",
            "Epoch 33/35\n",
            "167/167 [==============================] - ETA: 0s - loss: 0.0131 - accuracy: 0.9295\n",
            "Epoch 00033: val_accuracy improved from 0.92872 to 0.93028, saving model to output/transformer/weights.h5\n",
            "167/167 [==============================] - 37s 219ms/step - loss: 0.0131 - accuracy: 0.9295 - val_loss: 1.8743 - val_accuracy: 0.9303\n",
            "Epoch 34/35\n",
            "167/167 [==============================] - ETA: 0s - loss: 0.0119 - accuracy: 0.9310\n",
            "Epoch 00034: val_accuracy improved from 0.93028 to 0.93175, saving model to output/transformer/weights.h5\n",
            "167/167 [==============================] - 37s 219ms/step - loss: 0.0119 - accuracy: 0.9310 - val_loss: 1.8762 - val_accuracy: 0.9318\n",
            "Epoch 35/35\n",
            "167/167 [==============================] - ETA: 0s - loss: 0.0112 - accuracy: 0.9325\n",
            "Epoch 00035: val_accuracy improved from 0.93175 to 0.93315, saving model to output/transformer/weights.h5\n",
            "167/167 [==============================] - 37s 219ms/step - loss: 0.0112 - accuracy: 0.9325 - val_loss: 1.9054 - val_accuracy: 0.9331\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gICGO5ydePU4"
      },
      "source": [
        "DATA_OUT_PATH = 'output/'\n",
        "SAVE_FILE_NM = 'weights.h5'\n",
        "\n",
        "model.load_weights(os.path.join(DATA_OUT_PATH, model_name, SAVE_FILE_NM))"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsix3s_-e-4_",
        "outputId": "ce16c584-1d65-47dd-9656-a4f3842a35c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "char2idx = prepro_configs['char2idx']\n",
        "idx2char = prepro_configs['idx2char']\n",
        "\n",
        "text = \"    ?\"\n",
        "test_index_inputs, _ = enc_processing([text], char2idx)\n",
        "outputs = model.inference(test_index_inputs)\n",
        "\n",
        "print(' '.join([idx2char[str(o)] for o in outputs]))"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
